{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b81cf2-8603-42bd-995e-9e14631effd0",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of MLTransform. MLTransform is a PTransform that is used to wrap data processing transforms provided by Beam. The data processing transforms are useful to process the data going into the ML training/inference work. \n",
    "\n",
    "In the notebook, we will be making use of data processing transforms defined at `apache_beam/ml/transforms/tft`. These modules are implemented using `tensorflow_transform` but all the details of `tensorflow_transform` are abstracted from the end user. Users can simply pass the data, which could be a dictionary where keys being the name of the columns and values being either scalar, lists or numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0097dbd-2657-4cbe-a334-e0401816db01",
   "metadata": {},
   "source": [
    "Import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88ddd3a4-3643-4731-b99e-a5d697fbc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.ml.transforms.base import MLTransform\n",
    "from apache_beam.ml.transforms.tft import ComputeAndApplyVocabulary\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.ml.transforms.utils import ArtifactsFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdabbc57-ec98-4113-b37e-61962f488d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_location = './my_artifacts'\n",
    "# Remove this code\n",
    "def delete_artifact_location(artifact_location):\n",
    "    import shutil\n",
    "    import os\n",
    "    if os.path.exists(artifact_location):\n",
    "        shutil.rmtree(artifact_location)\n",
    "import logging\n",
    "for _ in (\"tensorflow_transform\"):\n",
    "    logging.getLogger(_).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1719c-7287-4cec-870b-9fabc4c4a4ef",
   "metadata": {},
   "source": [
    "## ComputeAndApplyVocabulary\n",
    "Let us compute vocabulary for the dataset and map its vocabulary to unique index. This can be achieved using `ComputeAndApplyVocabulary`(LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56d6d09a-8d34-444f-a1e4-a75624b36932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/usr/local/google/home/anandinguva/.local/share/jupyter/runtime/kernel-72c846fd-7a7d-4e99-8978-4a61b898e7b0.json']\n",
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are outputting instance dicts from `TransformDataset` which will not provide optimal performance. Consider setting  `output_record_batches=True` to upgrade to the TFXIO format (Apache Arrow RecordBatch). Encoding functionality in this module works with both formats.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/usr/local/google/home/anandinguva/.local/share/jupyter/runtime/kernel-72c846fd-7a7d-4e99-8978-4a61b898e7b0.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/6c0885c2e7f148369a4ceeed08f0b137/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/6c0885c2e7f148369a4ceeed08f0b137/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/6f0a3df927ed4e2d977625225c6e04dc/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/6f0a3df927ed4e2d977625225c6e04dc/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(x=array([1, 0, 4]))\n",
      "Row(x=array([1, 0, 6, 2, 3, 5]))\n"
     ]
    }
   ],
   "source": [
    "delete_artifact_location(artifact_location)\n",
    "data = [\n",
    "    {'x': ['I', 'love', 'pie']},\n",
    "    {'x': ['I', 'love', 'going', 'to', 'the', 'park']}\n",
    "]\n",
    "options = PipelineOptions()\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    data = (\n",
    "        p \n",
    "        | beam.Create(data)\n",
    "        | MLTransform(artifact_location=artifact_location).with_transform(ComputeAndApplyVocabulary(columns=['x']))\n",
    "        | beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057eacf1-54b4-4097-a24c-d4b9bb34c8d0",
   "metadata": {},
   "source": [
    "On the same data, let us use `TFIDF`(LINK) after we compute vocab indices for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e133002-7229-459d-8e3c-b41f4d65e76d",
   "metadata": {},
   "source": [
    "## Fetching vocabulary artifacts\n",
    "\n",
    "To fetch artifacts generated by the `ComputeAndApplyVocabulary`, in this case a file with all the vocabulary in the dataset, use `ArtifactsFetcher` class that will fetch vocab list, path to the vocab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c5fe46a-c718-4a82-bad8-aa091c0b0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'I', 'to', 'the', 'pie', 'park', 'going']\n",
      "./my_artifacts/transform_fn/assets/compute_and_apply_vocab\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_vocab_size() missing 1 required positional argument: 'vocab_filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocab_file_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# get vocab size\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[43mfetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(vocab_size)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_vocab_size() missing 1 required positional argument: 'vocab_filename'"
     ]
    }
   ],
   "source": [
    "fetcher = ArtifactsFetcher(artifact_location=artifact_location)\n",
    "# get vocab list\n",
    "vocab_list = fetcher.get_vocab_list()\n",
    "print(vocab_list)\n",
    "# get vocab file path\n",
    "vocab_file_path = fetcher.get_vocab_filepath()\n",
    "print(vocab_file_path)\n",
    "# get vocab size\n",
    "vocab_size = fetcher.get_vocab_size()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f955f3d-3192-42f7-aa55-48249223418d",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a8cb94b-57eb-4c4c-aa4c-22cf3193ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.ml.transforms.tft import TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "970d7222-194e-460e-b698-a00f1fcafb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/usr/local/google/home/anandinguva/.local/share/jupyter/runtime/kernel-72c846fd-7a7d-4e99-8978-4a61b898e7b0.json']\n",
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are outputting instance dicts from `TransformDataset` which will not provide optimal performance. Consider setting  `output_record_batches=True` to upgrade to the TFXIO format (Apache Arrow RecordBatch). Encoding functionality in this module works with both formats.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/usr/local/google/home/anandinguva/.local/share/jupyter/runtime/kernel-72c846fd-7a7d-4e99-8978-4a61b898e7b0.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/0a6a470f382c455fb68b594737de8e5f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/0a6a470f382c455fb68b594737de8e5f/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/d0e70fe851d94d03b924ab1a769fa964/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/d0e70fe851d94d03b924ab1a769fa964/assets\n",
      "WARNING:absl:Analyzer (tfidf/sum/temporary_analyzer_output/PlaceholderWithDefault:0) node's cache key varies on repeated tracing. This warning is safe to ignore if you either specify `name` for all analyzers or if the order in which they are invoked is deterministic. If not, please file a bug with details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/de7237d2106440a4b2080bf340a5562b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/de7237d2106440a4b2080bf340a5562b/assets\n",
      "WARNING:absl:Analyzer (tfidf/sum/temporary_analyzer_output/PlaceholderWithDefault:0) node's cache key varies on repeated tracing. This warning is safe to ignore if you either specify `name` for all analyzers or if the order in which they are invoked is deterministic. If not, please file a bug with details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(x=array([1, 0, 4]), x_tfidf_weight=array([0.33333334, 0.33333334, 0.4684884 ], dtype=float32), x_vocab_index=array([0, 1, 4]))\n",
      "Row(x=array([1, 0, 6, 2, 3, 5]), x_tfidf_weight=array([0.16666667, 0.16666667, 0.2342442 , 0.2342442 , 0.2342442 ,\n",
      "       0.2342442 ], dtype=float32), x_vocab_index=array([0, 1, 2, 3, 5, 6]))\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'x': ['I', 'love', 'pie']},\n",
    "    {'x': ['I', 'love', 'going', 'to', 'the', 'park']}\n",
    "]\n",
    "delete_artifact_location(artifact_location)\n",
    "options = PipelineOptions()\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    data = (\n",
    "        p \n",
    "        | beam.Create(data)\n",
    "        | MLTransform(artifact_location=artifact_location\n",
    "                     ).with_transform(ComputeAndApplyVocabulary(columns=['x'])\n",
    "                     ).with_transform(TFIDF(columns=['x']))\n",
    "    )\n",
    "    _ = data | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1feb4f-bb0b-4f61-8349-e1ba411858cf",
   "metadata": {},
   "source": [
    "TFIDF provides two artifacts. These are provided in the output suffixed with `tfidf_weight` and `vocab_index` to the processing column name. \n",
    "\n",
    "* `vocab_index`: indices of the words computed in the `ComputeAndApplyVocabulary`.\n",
    "* `tfidif_weight`: weight for each vocab index. The weight represents how important the word present at that vocab_index is to the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5b9dd-ed35-460b-9fb3-0ffb5c3633db",
   "metadata": {},
   "source": [
    "## ScaleTo01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd20692-6d14-4ece-a2e7-69a2a6fac5d4",
   "metadata": {},
   "source": [
    "Now let us scale the data to be in the range of 0 and 1. This would be done by calculating `min` and `max` values on the whole dataset and then performing\n",
    "```\n",
    "x = (x - x_min) / (x_max)\n",
    "```\n",
    "\n",
    "This can be achieved using MLTransform and ScaleTo01 data processing transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6cbff40-b993-4014-8231-c90ba8b14f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.ml.transforms.tft import ScaleTo01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "841a8e1f-2f5b-4fd9-bb35-12a2393922de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:absl:You are outputting instance dicts from `TransformDataset` which will not provide optimal performance. Consider setting  `output_record_batches=True` to upgrade to the TFXIO format (Apache Arrow RecordBatch). Encoding functionality in this module works with both formats.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/4a0a3683ae774a74b877a27e48d1f796/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/4a0a3683ae774a74b877a27e48d1f796/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/8c6d7a67caa44232807f1d9d251e9ede/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_artifacts/tftransform_tmp/8c6d7a67caa44232807f1d9d251e9ede/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(x=array([0.        , 0.01010101, 0.02020202], dtype=float32), x_max=array([100.], dtype=float32), x_min=array([1.], dtype=float32))\n",
      "Row(x=array([0.03030303, 0.04040404, 0.06060606], dtype=float32), x_max=array([100.], dtype=float32), x_min=array([1.], dtype=float32))\n",
      "Row(x=array([0.09090909, 0.01010101, 0.09090909, 0.33333334, 1.        ,\n",
      "       0.53535354, 0.1919192 , 0.09090909, 0.01010101, 0.02020202,\n",
      "       0.1010101 , 0.11111111], dtype=float32), x_max=array([100.], dtype=float32), x_min=array([1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {'x': [1, 2, 3]}, {'x': [4, 5, 7]}, {'x': [10, 2, 10, 34, 100, 54, 20, 10, 2, 3, 11, 12]}]\n",
    "\n",
    "# delete_artifact_location(artifact_location)\n",
    "with beam.Pipeline() as p:\n",
    "    _ = (\n",
    "        p \n",
    "        | 'CreateData' >> beam.Create(data)\n",
    "        | 'MLTransform' >> MLTransform(artifact_location=artifact_location).with_transform(ScaleTo01(columns=['x']))\n",
    "        | 'PrintResults' >> beam.Map(print)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1838ecb-2168-45f8-bdf2-41ae0007cb71",
   "metadata": {},
   "source": [
    "The output would be the values that are scaled between 0 and 1. Scaling is done using max and min values computed from the entire dataset.\n",
    "Also, the output will comprise of artifacts such as `x_max`, `x_min`, which represents the max and min values of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e382cca-cfd3-4ac1-956a-16480603dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
